{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Model Selection\n",
    "\n",
    "The idea of active model selection is the process of generating data points and asking the user to label them in order to help the user select a model among a set of _k_ candidate models.  An automated machine learning process can often provide a set of candidate models to solve a domain scientists needs.  However, there are several issues with expecting the domain scientist to simply take the top-ranked candidate model and using it in production, or training it further.  First, the SME has to trust that the autoML process actually executed correctly and did something intelligent.  While we know that these algorithms can and should work, there is no proof of correctness, and we are well aware that there can be errors resulting in nonsensical models.  Second, even if the autoML process worked correctly, there could have been some bias present in the training set.  To remedy these issues, the SME needs to interact with each model to get a feel for how they make predictions on domain data.\n",
    "\n",
    "Previous efforts towards model selection, such as TreePOD, show the predictions of the candidate models on the training or validation parts of the dataset.  However, that might not solve the issue of revealing bias in the training process; in fact, it may reinforce it, since model selection is occurring on the same source of data.  SMEs expect that resulting models will be robust, and that they agree with their intuition.  \n",
    "\n",
    "Active Model Selection aids in model selection by generating data points and asking the SME to provide labels for those data points.  Then, the SME is shown the set of models that agree with their own judgment.  By comparing their predictions, they are able to determine which models agree with their judgments, as well as gaining trust in the resulting models.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "Assuming that there are _n_ data points, {x_i} = D, with _m_ features, and _k_ potential models.  \n",
    "\n",
    "    for i in 0, ...\n",
    "        d_i = generate_data_point(D \\cup {d_0, ..., d_i})\n",
    "        y_i = active_query(d_i)\n",
    "        \n",
    "Our contribution is a novel way of generating data points.  We can compare it to randomly generated data points, as well as some other naive methods of generating data points.  \n",
    "\n",
    "Our method is to build a random variable _h_ representing the __entropy__ of the set of predictions of the available models which has a gaussian process prior, based on the entropy of the predictions on the training set and all labeled points.  \n",
    "\n",
    "This method is reminiscent of interpretability methods which make approximations to complex models to explain them.  Gaussian processes could be used to make approximations of what a model would predict.  However, we aren't using a GP to approximate a model, we are using it to predict which spaces of the data are most disagreed upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylancashman/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "TRAINING_SET_CT = 50\n",
    "VALIDATION_SET_CT = 20\n",
    "NROWS = TRAINING_SET_CT + VALIDATION_SET_CT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "training_df = pd.read_csv('adult_data/adult.csv', sep='\\s*,\\s*', nrows=NROWS, usecols=['education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'age', 'label'])\n",
    "\n",
    "training_labels = training_df['label'][:TRAINING_SET_CT]\n",
    "training_predictors = training_df.drop(columns=('label'))[0:TRAINING_SET_CT]\n",
    "\n",
    "val_labels = training_df['label'][TRAINING_SET_CT:]\n",
    "val_predictors = training_df.drop(columns=('label'))[TRAINING_SET_CT:]\n",
    "\n",
    "\n",
    "# training_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 score 0.86\n",
      "m2 score 0.98\n",
      "m1 score 0.86\n",
      "m2 score 1.0\n",
      "m1 score 0.86\n",
      "m2 score 1.0\n",
      "m1 score 0.86\n",
      "m2 score 1.0\n",
      "m1 score 0.86\n",
      "m2 score 1.0\n",
      "uniform_knn score 0.84\n",
      "distance_knn score 1.0\n",
      "uniform_knn score 0.78\n",
      "distance_knn score 1.0\n",
      "uniform_knn score 0.78\n",
      "distance_knn score 1.0\n",
      "uniform_knn score 0.78\n",
      "distance_knn score 1.0\n",
      "uniform_knn score 0.78\n",
      "distance_knn score 1.0\n",
      "gini_dt score 0.78\n",
      "entropy_dt score 0.78\n",
      "gini_dt score 0.86\n",
      "entropy_dt score 0.84\n",
      "gini_dt score 0.9\n",
      "entropy_dt score 0.9\n",
      "gini_dt score 0.92\n",
      "entropy_dt score 0.9\n",
      "gini_dt score 0.94\n",
      "entropy_dt score 0.92\n",
      "nb_1 score 0.34\n",
      "nb_2 score 0.28\n",
      "nb_1 score 0.34\n",
      "nb_2 score 0.28\n",
      "nb_1 score 0.34\n",
      "nb_2 score 0.28\n",
      "nb_1 score 0.34\n",
      "nb_2 score 0.28\n",
      "nb_1 score 0.34\n",
      "nb_2 score 0.28\n"
     ]
    }
   ],
   "source": [
    "# Build random models\n",
    "model_dict = {}\n",
    "\n",
    "# We build 10 models each of:\n",
    "#  - logistic regression\n",
    "#  - SVM\n",
    "#  - kNN\n",
    "#  - Decision Tree\n",
    "#  - Naive Bayes\n",
    "\n",
    "# Logistic regression\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "for i, C in enumerate(np.logspace(0, 1, num=5)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(training_predictors, training_labels)\n",
    "    clf_l2_LR.fit(training_predictors, training_labels)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "#     print(\"C=%.2f\" % C)\n",
    "#     print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "#     print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(predictors, labels))\n",
    "#     print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "#     print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(predictors, labels))\n",
    "    \n",
    "    model_dict[\"logreg_c%.2f_l1\" % C] = clf_l1_LR\n",
    "    model_dict[\"logreg_c%.2f_l2\" % C] = clf_l2_LR\n",
    "\n",
    "#  SVM\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py\n",
    "from sklearn import svm\n",
    "for i, C in enumerate(np.logspace(0, 1, num=5)):\n",
    "    m1 = svm.SVC(kernel='linear', C=C)\n",
    "    m2 = svm.SVC(kernel='rbf', gamma=0.7, C=C)\n",
    "    \n",
    "    m1.fit(training_predictors, training_labels)\n",
    "    m2.fit(training_predictors, training_labels)\n",
    "    print(\"m1 score\", m1.score(training_predictors, training_labels))\n",
    "    print(\"m2 score\", m2.score(training_predictors, training_labels))\n",
    "\n",
    "    model_dict[\"svm_c%.2f_linear\" % C] = m1\n",
    "    model_dict[\"svm_c%.2f_rbf\" % C] = m2\n",
    "    \n",
    "# kNN\n",
    "# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "from sklearn import neighbors\n",
    "for i in range(5):\n",
    "    k = 2 + (2 * i)\n",
    "    uniform_knn = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "    distance_knn = neighbors.KNeighborsClassifier(k, weights='distance')\n",
    "    uniform_knn.fit(training_predictors, training_labels)\n",
    "    distance_knn.fit(training_predictors, training_labels)\n",
    "    print(\"uniform_knn score\", uniform_knn.score(training_predictors, training_labels))\n",
    "    print(\"distance_knn score\", distance_knn.score(training_predictors, training_labels))\n",
    "\n",
    "    model_dict[\"knn_k%.2f_uniform\" % k] = uniform_knn\n",
    "    model_dict[\"knn_k%.2f_distance\" % k] = distance_knn\n",
    "    \n",
    "# Decision Tree\n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py\n",
    "from sklearn import tree\n",
    "for i in range(5):\n",
    "    max_depth = 2 + i\n",
    "    gini_dt = tree.DecisionTreeClassifier(criterion='gini', max_depth=max_depth)\n",
    "    entropy_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "\n",
    "    gini_dt.fit(training_predictors, training_labels)\n",
    "    entropy_dt.fit(training_predictors, training_labels)\n",
    "    print(\"gini_dt score\", gini_dt.score(training_predictors, training_labels))\n",
    "    print(\"entropy_dt score\", entropy_dt.score(training_predictors, training_labels))\n",
    "\n",
    "    model_dict[\"dt_d%.2f_gini\" % max_depth] = gini_dt\n",
    "    model_dict[\"dt_d%.2f_entropy\" % max_depth] = entropy_dt\n",
    "\n",
    "# Naive Bayes\n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes\n",
    "from sklearn import naive_bayes\n",
    "for i, smoothing in enumerate(np.logspace(1e-10, 1e-7, num=5)):\n",
    "    nb_1 = naive_bayes.GaussianNB(priors=(0.8, 0.2))\n",
    "    nb_2 = naive_bayes.GaussianNB(priors=(0.2, 0.8))\n",
    "\n",
    "    nb_1.fit(training_predictors, training_labels)\n",
    "    nb_2.fit(training_predictors, training_labels)\n",
    "    print(\"nb_1 score\", nb_1.score(training_predictors, training_labels))\n",
    "    print(\"nb_2 score\", nb_2.score(training_predictors, training_labels))\n",
    "\n",
    "    model_dict[\"nb1_s%.2f\" % smoothing] = nb_1\n",
    "    model_dict[\"nb2_s%.2f\" % smoothing] = nb_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.286397\n",
       "1     0.286397\n",
       "2     0.909736\n",
       "3     0.998196\n",
       "4     0.992774\n",
       "5     0.609840\n",
       "6     0.168661\n",
       "7     0.609840\n",
       "8     0.286397\n",
       "9     0.998196\n",
       "10    0.954434\n",
       "11    0.168661\n",
       "12    0.286397\n",
       "13    0.983708\n",
       "14    0.286397\n",
       "15    0.286397\n",
       "16    0.286397\n",
       "17    0.286397\n",
       "18    0.848548\n",
       "19    0.286397\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate entropies\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "predictions = {}\n",
    "for name, m in model_dict.items():\n",
    "    predictions[name] = m.predict(val_predictors)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df\n",
    "entropies = predictions_df.apply(lambda r: stats.entropy([x for x in Counter(r).values()], base=2), axis=1)\n",
    "entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessRegressor(alpha=1e-10, copy_X_train=True, kernel=None,\n",
       "             n_restarts_optimizer=0, normalize_y=False,\n",
       "             optimizer='fmin_l_bfgs_b', random_state=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we build the GP estimator for entropy over the whole space.\n",
    "from sklearn import gaussian_process\n",
    "gp = gaussian_process.GaussianProcessRegressor()\n",
    "gp.fit(val_predictors, entropies)\n",
    "\n",
    "gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.5",
   "language": "python",
   "name": "3.6.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
